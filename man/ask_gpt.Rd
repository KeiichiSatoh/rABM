% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ask_gpt.R
\name{ask_gpt}
\alias{ask_gpt}
\title{Ask GPT with Code-Aware Retrieval-Augmented Generation (RAG)}
\usage{
ask_gpt(
  question,
  gpt = c("openai", "bedlock", "databricks", "google_vertex", "ollama"),
  overwrite_rag = FALSE,
  system_prompt = NULL,
  include_default_system_prompt = TRUE,
  gpt_args = list(),
  embed_args = list(model = "text-embedding-3-small")
)
}
\arguments{
\item{question}{A character string. The initial question to start the conversation.}

\item{gpt}{A character string indicating which GPT engine to use.
One of \code{"openai"}, \code{"bedlock"}, \code{"databricks"}, \code{"google_vertex"}, or \code{"ollama"}.}

\item{overwrite_rag}{Logical. If \code{TRUE}, the existing local knowledge store (DuckDB file) will be rebuilt. Default is \code{FALSE}.}

\item{system_prompt}{Optional character vector of additional system prompts. Default is \code{NULL}.}

\item{include_default_system_prompt}{Logical. If \code{TRUE} (default),
a default prompt is included to guide the assistant's behavior as an R programming expert.}

\item{gpt_args}{A named list of arguments to be passed to the \verb{chat_*} function
(e.g., model name, temperature, etc.).}

\item{embed_args}{A named list of arguments to pass to the embedding function.
For example, \code{list(model = "text-embedding-3-small")}.}
}
\value{
A data frame with two columns:
\describe{
\item{question}{User-provided queries}
\item{answer}{Model-generated responses}
}
}
\description{
This function allows users to interactively query an LLM (via \code{ellmer}),
using Retrieval-Augmented Generation (RAG) based on the content of the rABM
It fetches and embeds relevant package files (R scripts, tests, docs), stores them locally,
and initiates a chat loop with automatic retrieval from the embedded store.
}
\details{
This function supports RAG-based querying by building or loading a local
DuckDB knowledge store.
The store is constructed from a rABM's GitHub repository containing R package source files,
and embeddings are created via \verb{ragnar::embed_*()} functions.

During the REPL-like interaction, relevant content is retrieved from the store
using \code{ragnar_retrieve()}
and passed into the prompt context. The interaction continues until
the user types \code{"END"}.

If the knowledge store does not exist and \code{overwrite_rag = FALSE},
the user is prompted to build it.
The resulting DuckDB file is named as \code{"rABM_ragnar_<gpt>_duckdb"} and
saved to the working directory.
}
\section{Important Prerequisite}{

You must have a valid API key for the selected GPT backend.
For example, if you use OpenAI's ChatGPT, visit:
https://platform.openai.com/

to generate a key under Dashboard > API Keys.

Note that running this function may incur OpenAI API charges.

You may temporarily register the key via:

\code{Sys.setenv(OPENAI_API_KEY = "sk-...")}

Alternatively, you can store the API key in your .Renviron file to make it persist across sessions.

As a more convenient option, you can also pass the API key directly as an argument
In this case, set your api_key as arguments to gpt_args and embed_args:

\code{ask_gpt(question, gpt_args = list(api_key = ...), embed_args = list(api_key = ...))}
}

\examples{
\dontrun{
ask_gpt(
  question = "How do I initialize agents in rABM?",
  gpt = "openai",
  system_prompt = "Please answer in Japanese."
)
}
}
\seealso{
\href{https://ellmer.tidyverse.org/}{ellmer} for chat-related functionality. \cr
\href{https://ragnar.tidyverse.org/}{ragnar} for RAG-related functionality.
}
